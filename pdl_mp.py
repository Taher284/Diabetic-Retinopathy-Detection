# -*- coding: utf-8 -*-
"""PDL_MP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AmIRHVpVNwU_gK0xuwI1wuzGRmbfYLvm
"""

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive"

import os
root_dir = "/content/drive/My Drive/gaussian_filtered_images"

!pip install efficientnet

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import efficientnet.tfkeras as efn

# Define the root directory containing subfolders of images
root_dir = "/content/drive/MyDrive/gaussian_filtered_images"
# Define parameters for image preprocessing and data augmentation
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # Splitting the data into training and validation sets
)

# Define the image data generators for training and validation
batch_size = 32
image_size = (224, 224)
train_generator = datagen.flow_from_directory(
    root_dir,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

validation_generator = datagen.flow_from_directory(
    root_dir,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

# Define the EfficientNet model
base_model = efn.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add a global average pooling layer and a dense output layer
x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
output = tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')(x)

# Combine the base model and the new layers into a new model
model = tf.keras.Model(inputs=base_model.input, outputs=output)

# Freeze the layers of the base model
base_model.trainable = False

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
epochs = 10
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size
)

import numpy as np
from tensorflow.keras.preprocessing import image


# Load the image you want to test
img_path = "/content/drive/MyDrive/gaussian_filtered_images/No_DR/005b95c28852.png"  # Replace 'path_to_your_image.jpg' with the path to your image file
img = image.load_img(img_path, target_size=(224, 224))  # Resize the image to match the input size of the model
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x /= 255.  # Normalize the image data

# Make predictions
predictions = model.predict(x)

# Get the predicted class label
#predicted_class = np.argmax(predictions)

# Print the predicted class label
#print("Predicted class:", predicted_class)

class_names = ['Mild', 'Moderate','Normal']  # Replace with actual class names used

# Prediction
pred = model.predict(x)
predicted_class = np.argmax(pred)
print(f"Predicted class: {class_names[predicted_class]}")

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Get true labels and predicted labels
y_true = validation_generator.classes
y_pred = model.predict(validation_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print(classification_report(y_true, y_pred_classes, target_names=class_names))

import matplotlib.pyplot as plt

# Get the training history from the History object
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
training_accuracy = history.history['accuracy']
validation_accuracy = history.history['val_accuracy']

# Plot the training and validation loss
plt.figure(figsize=(12, 6))
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot the training and validation accuracy
plt.figure(figsize=(12, 6))
plt.plot(training_accuracy, label='Training Accuracy')
plt.plot(validation_accuracy, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Plotting the training and validation loss
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plotting the training and validation accuracy
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

import os

model_path = "/content/drive/MyDrive/trained_model.h5"  # Adjust the path
print("File exists:", os.path.exists(model_path))

# Define the path where you want to save the trained model
model_save_path = "trained_model.h5"

# Save the trained model
model.save(model_save_path)

print("Model saved at:", model_save_path)

import tensorflow as tf
from tensorflow.keras.preprocessing import image
import numpy as np

# Load the trained model
# Changed 'path_to_your_saved_model.h5' with 'trained_model.h5'
model = tf.keras.models.load_model('trained_model.h5')

# Load and preprocess the image
img_path = "/content/drive/MyDrive/gaussian_filtered_images/Severe/03c85870824c.png"
img = image.load_img(img_path, target_size=(224, 224))  # Resize the image to match the input size of the model
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to create batch dimension
img_array = img_array / 255.0  # Normalize pixel values to [0, 1]

# Use the model to predict the class of the image
predictions = model.predict(img_array)

# Optionally, visualize the prediction result
predicted_class = np.argmax(predictions)
class_names = ['Severe', 'Mild', 'Normal']  # Replace with your actual class names
predicted_class_name = class_names[predicted_class]
print('Predicted class:', predicted_class_name)

""" # Define the checkpoint path where you want to save the model checkpoints
checkpoint_path = "model_checkpoint.h5"

# Define a callback to save the model checkpoints during training
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    save_weights_only=False,  # Save the entire model
    monitor='val_accuracy',   # Monitor validation accuracy to save the best model
    save_best_only=True,      # Save only the best model based on validation accuracy
    mode='max',               # Save the model with the highest validation accuracy
    verbose=1
) """

import numpy as np
from tensorflow.keras.preprocessing import image

# Load the trained model
model1 = tf.keras.models.load_model('trained_model.h5')  # Replace 'your_model.h5' with the path to your saved model file

# Load the image you want to test
img_path = '/content/drive/MyDrive/gaussian_filtered_images/Proliferate_DR/034cb07a550f.png'  # Replace 'path_to_your_image.jpg' with the path to your image file
img = image.load_img(img_path, target_size=(224, 224))  # Resize the image to match the input size of the model
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x /= 255.  # Normalize the image data

# Make predictions
predictions = model1.predict(x)

# Get the predicted class label
#predicted_class = np.argmax(predictions)

# Print the predicted class label
#print("Predicted class:", predicted_class)

predicted_class = np.argmax(predictions)
class_names = ['Severe', 'Mild', 'Normal']  # Replace with your actual class names
predicted_class_name = class_names[predicted_class]
print('Predicted class:', predicted_class_name)

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

class FixedDropout(tf.keras.layers.Dropout):
    def _get_noise_shape(self, inputs):
        if self.noise_shape is None:
            return self.noise_shape
        symbolic_shape = tf.shape(inputs)
        noise_shape = [symbolic_shape[axis] if shape is None else shape for axis, shape in enumerate(self.noise_shape)]
        return tuple(noise_shape)


model1 = tf.keras.models.load_model('trained_model.h5', custom_objects={'FixedDropout': FixedDropout})


img_paths = ['/content/drive/MyDrive/gaussian_filtered_images/Severe/05cd0178ccfe.png','/content/drive/MyDrive/gaussian_filtered_images/Mild/0369f3efe69b.png']

for img_path in img_paths:

    img = image.load_img(img_path, target_size=(224, 224))  # Resize the image to match the input size of the model
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x /= 255.


    predictions = model1.predict(x)


    #predicted_class = np.argmax(predictions)
    predicted_class = np.argmax(predictions)
    class_names = ['Severe', 'Mild', 'Normal']  # Replace with your actual class names
    predicted_class_name = class_names[predicted_class]
    print('Predicted class:', predicted_class_name)

    predicted_probability = np.max(predictions) * 100


    print("Predicted class:", predicted_class)
    print("Predicted probability:", predicted_probability, "%")


    plt.imshow(img)
    plt.title(f"Predicted class: {predicted_class}, Probability: {predicted_probability:.2f}%")
    plt.axis('off')
    plt.show()

# Define a simple CNN model
cnn_model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')
])

cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the CNN model
cnn_history = cnn_model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=10  # You can increase this based on performance needs
)

# Evaluate both models
cnn_loss, cnn_acc = cnn_model.evaluate(validation_generator, verbose=0)
effnet_loss, effnet_acc = model.evaluate(validation_generator, verbose=0)

print(f"CNN Accuracy: {cnn_acc * 100:.2f}%")
print(f"EfficientNet Accuracy: {effnet_acc * 100:.2f}%")